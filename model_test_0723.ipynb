{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/mysite/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer,util\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from numpy.linalg import norm\n",
    "from numpy import dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '학식', 1: '편의시설', 2: '연락처', 3: '학사일정', 4: '공지사항', 5: '학교 날씨', 6: '학교 정보', 7: '졸업 요건', 8: '열람실'}\n"
     ]
    }
   ],
   "source": [
    "intent_to_num={\"학식\":0,\"편의시설\":1,\"연락처\":2,\"학사일정\":3,\"공지사항\":4,\"학교 날씨\":5,\"학교 정보\":6,\"졸업 요건\":7,\"열람실\":8}\n",
    "num_to_intent={value:key for key,value in intent_to_num.items()}\n",
    "print(num_to_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   의도          질문\n",
      "0  학식  오늘 학식을 알려줘\n",
      "1  학식  오늘 아침을 알려줘\n",
      "2  학식  오늘 점심을 알려줘\n",
      "3  학식  오늘 저녁을 알려줘\n",
      "4  학식   오늘 학식 뭐나와\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"dataset/의도분류질문.xlsx\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SentenceTransformer(\"snunlp/KR-SBERT-V40K-klueNLI-augSTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 8, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "sentences=[df.iloc[i,1] for i in range(len(df))]\n",
    "sentences_class=[intent_to_num[df.iloc[i,0]] for i in range(len(df))]\n",
    "print(sentences_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([166, 23, 768])\n",
      "Cosine Similarity: 0.803135922068623\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "# KoBERT 모델 및 토크나이저 로드\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "\n",
    "def embed_sentence(sentence, keyword_weights):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    token_embeddings = outputs.last_hidden_state.squeeze(0)\n",
    "    \n",
    "    token_ids = inputs['input_ids'].squeeze()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    token_weights = np.array([keyword_weights.get(token, 1.0) for token in tokens])\n",
    "    \n",
    "    weighted_embeddings = token_embeddings * torch.tensor(token_weights).unsqueeze(1).to(token_embeddings.device)\n",
    "    return weighted_embeddings.detach().cpu()\n",
    "\n",
    "def pad_sequences(sequences, maxlen=None, dtype=torch.float32, padding_value=0.0):\n",
    "    if maxlen is None:\n",
    "        maxlen = max(seq.size(0) for seq in sequences)\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if seq.size(0) < maxlen:\n",
    "            padding = torch.full((maxlen - seq.size(0), seq.size(1)), padding_value, dtype=dtype)\n",
    "            padded_seq = torch.cat([seq, padding], dim=0)\n",
    "        else:\n",
    "            padded_seq = seq\n",
    "        padded_sequences.append(padded_seq)\n",
    "    return torch.stack(padded_sequences)\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    flattened_vec1 = vec1.flatten()\n",
    "    flattened_vec2 = vec2.flatten()\n",
    "    return np.dot(flattened_vec1, flattened_vec2) / (np.linalg.norm(flattened_vec1) * np.linalg.norm(flattened_vec2))\n",
    "\n",
    "keyword_weights = {\n",
    "    \"총장\": 1.2,\n",
    "    \"학식\": 1.2, \n",
    "}\n",
    "\n",
    "embedding_vectors = [embed_sentence(sentence, keyword_weights) for sentence in sentences]\n",
    "embedding_vectors = pad_sequences(embedding_vectors)\n",
    "print(embedding_vectors.shape)\n",
    "\n",
    "similarity = cosine_similarity(embedding_vectors[1], embedding_vectors[2])\n",
    "print(f\"Cosine Similarity: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertModel' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embedding_vectors\u001b[38;5;241m=\u001b[39m[\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences]\n\u001b[1;32m      2\u001b[0m embedding_vectors\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(embedding_vectors)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(embedding_vectors\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mysite/lib/python3.12/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertModel' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "# embedding_vectors=[model.encode(sentence) for sentence in sentences]\n",
    "# embedding_vectors=torch.tensor(embedding_vectors)\n",
    "# print(embedding_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(A, B):\n",
    "    return dot(A, B) / (norm(A) * norm(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6t/w80zhm5565j25gfph102_yfm0000gn/T/ipykernel_18559/945312013.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  encoded_sentence=torch.tensor(encoded_sentence)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, '오늘 학식을 알려줘', 1.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(sentence):\n",
    "    # print(f\"원래 문장 : {sentence}\")\n",
    "    encoded_sentence=embed_sentence(sentence, keyword_weights)\n",
    "    embedding_vectors = pad_sequences([encoded_sentence])\n",
    "    encoded_sentence=torch.tensor(encoded_sentence)\n",
    "    cos_sim=cosine_similarity(encoded_sentence,embedding_vectors)\n",
    "    # print(f\"가장 높은 코사인 유사도 idx: {int(np.argmax(cos_sim))}\")\n",
    "\n",
    "    best_sim_idx=int(np.argmax(cos_sim))\n",
    "    selected_question=sentences[best_sim_idx]\n",
    "    # print(f\"선택된 질문 : {selected_question}\")\n",
    "    # print(f\"선태괸 질문과의 유사도 : {float(cos_sim.max())}\")\n",
    "    # print(f\"선택된 질문의 class : {num_to_intent[sentences_class[best_sim_idx]]}\")\n",
    "    \n",
    "    return sentences_class[best_sim_idx], selected_question,float(cos_sim.max())\n",
    "    # return int(np.argmax(cos_sim))\n",
    "\n",
    "s = \"학교 교가는 뭐야?\"\n",
    "predict(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6t/w80zhm5565j25gfph102_yfm0000gn/T/ipykernel_38451/1184705647.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  encoded_sentence=torch.tensor(encoded_sentence)\n"
     ]
    }
   ],
   "source": [
    "s = \"전화번호가 궁금해?\"\n",
    "\n",
    "import time\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "predict(s)\n",
    "\n",
    "end_time=time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"dataset/my_data_0702.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4487 [00:00<?, ?it/s]/var/folders/6t/w80zhm5565j25gfph102_yfm0000gn/T/ipykernel_38451/1184705647.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  encoded_sentence=torch.tensor(encoded_sentence)\n",
      "100%|██████████| 4487/4487 [03:16<00:00, 22.85it/s]\n"
     ]
    }
   ],
   "source": [
    "f=open(\"failure.txt\",'w')\n",
    "correct=0\n",
    "min_cos_sim=1000\n",
    "max_cos_sim=-1000\n",
    "for i in tqdm(range(len(data))):\n",
    "    label=data.iloc[i]['label']-2 if data.iloc[i]['label'] >=8 else data.iloc[i]['label'] -1\n",
    "    predict_class, pred_str,best_cos_sim=predict(data.iloc[i]['sentence'])\n",
    "\n",
    "    if predict_class==label:\n",
    "        min_cos_sim=min(min_cos_sim,best_cos_sim)\n",
    "        correct+=1\n",
    "    else:\n",
    "        max_cos_sim=max(max_cos_sim,best_cos_sim)\n",
    "        f.write(f\"원래 문장 : {data.iloc[i]['sentence']}, 원래 라벨 : {label}, 예측 라벨 : {predict_class}\\n\")\n",
    "        f.write(f\"예측한 문장 : {pred_str}, 코사인 유사도 : {best_cos_sim}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도 : 11.23244929797192 % \n",
      "최저 유사도 : 0.9999999999999998\n",
      "최고 유사도 : 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "print(f\"예측 정확도 : {correct/len(data)*100} % \")\n",
    "print(f\"최저 유사도 : {min_cos_sim}\")\n",
    "print(f\"최고 유사도 : {max_cos_sim}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
